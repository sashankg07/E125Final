<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Without Bias: Training and Simmulation</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f9;
      color: #333;
      line-height: 1.6;
    }
    header {
      background-color: #3f51b5;
      color: white;
      padding: 20px;
      text-align: center;
    }
    main {
      max-width: 900px;
      margin: 20px auto;
      padding: 20px;
      background: white;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
    }
    section {
      margin-bottom: 40px;
    }
    h2 {
      color: #3f51b5;
    }
    .button {
      display: inline-block;
      background-color: #3f51b5;
      color: white;
      padding: 10px 20px;
      text-decoration: none;
      border-radius: 5px;
      cursor: pointer;
    }
    .button:hover {
      background-color: #303f9f;
    }
    .output {
      margin-top: 20px;
      padding: 10px;
      border-radius: 5px;
      background-color: #e3f2fd;
      border: 1px solid #bbdefb;
    }
    .chart-container {
      margin: 20px 0;
    }
    canvas {
      max-width: 100%;
    }
    footer {
      text-align: center;
      padding: 10px;
      background-color: #3f51b5;
      color: white;
      margin-top: 40px;
    }
  </style>
</head>
<body>
  <header>
    <h1>AI Without Bias: Simmulation and Case Studies</h1>
    <p>Exploring the challenges and solutions to bias in artificial intelligence.</p>
  </header>
  <main>
    <section>
      <h2>What Is Bias in AI & Why Is It Important?</h2>
      <p>
        Bias in AI is the unfair preferential treatment of individuals or groups of poeple because of flaws in the AI systems.
        It typically stems from historical, societal, or data biases present during the creation of the model and can lead to unintended consequences in decision-making.
      </p>
      <p>Bias is a very relavent issue, especially with the growing prelavence of AI. There is a growing need for fairness and accountability in technology. When AI is biased from their training data or algorithms, it risks perpetuating or amplifying societal injustice. Understanding and addressing bias in AI is essential to ensure that these technologies promote inclusivity and ethical decision-making. </p>
    </section>

    

    <section>
      <h2>Interactive Simulation: Bias in Hiring Outcomes</h2>
      <p>
        Adjust the bias level and observe its impact on hiring outcomes for candidates of different genders. 
        Click the button multiple times with different levels of bias and see how easy/hard it is to identify whether the algorithm is biased or not.
        Play around with the bias level and try again, see what conclusions you can draw from this.
        <br><br>
        Keep in mind that this is a simmulation of 100 candidates, with genders assigned randomly (approximately 50% male and 50% female depending on the run). Each candidate has an 80% chance of being "qualified" for hiring. The number of male and female hires is displayed in a bar chart and summary, updating dynamically with each run.
      </p>
      <div class="controls">
        <label for="bias">Bias Level (Middle = Neutral, Either Side = Fully Biased):</label>
        <input type="range" id="bias" min="0" max="1" step="0.1" value="0.5" />
      </div>
      <button class="button" onclick="simulate()">Run Simulation</button>
      <div class="output" id="output"></div>
      <div class="chart-container">
        <canvas id="chart" width="400" height="200"></canvas>
      </div>
    </section>

    

    <table border="1" style="border-collapse: collapse; width: 100%; text-align: left;">
        <thead>
          <tr style="background-color: #f4f4f9;">
            <th style="padding: 10px;">Key Takeaway</th>
            <th style="padding: 10px;">Explanation</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td style="padding: 10px;"><strong>Bias Is Hard to Detect Without Iteration</strong></td>
            <td style="padding: 10px;">Small shifts in bias are difficult to identify without running the algorithm over multiple cycles. Patterns only emerge when outcomes are analyzed over time, requiring repeated audits and statistical analysis.</td>
          </tr>
          <tr>
            <td style="padding: 10px;"><strong>Complexity of Variables</strong></td>
            <td style="padding: 10px;">Results depend on many factors, such as the proportion of qualified candidates, the total number of candidates, and hiring thresholds. These variables interact with bias, complicating its detection.</td>
          </tr>
          <tr>
            <td style="padding: 10px;"><strong>Cumulative Impact of Small Biases</strong></td>
            <td style="padding: 10px;">Even minor biases in decision-making can compound over time, leading to significant disparities after many iterations.</td>
          </tr>
          <tr>
            <td style="padding: 10px;"><strong>Interplay of Proxy Variables</strong></td>
            <td style="padding: 10px;">Variables like experience, education, or hobbies can act as proxies for sensitive attributes, making it harder to see how bias affects outcomes.</td>
          </tr>
          <tr>
            <td style="padding: 10px;"><strong>Real-World Implications</strong></td>
            <td style="padding: 10px;">This simulation reflects how unchecked hiring algorithms can unintentionally reinforce systemic inequalities, highlighting the importance of regular auditing and bias mitigation strategies.</td>
          </tr>
        </tbody>
      </table>
      
      <section>
        <h2>Case Studies</h2>
        <h3>Case Study 1: COMPAS Risk Assessment</h3>
        <p>
          The COMPAS tool, in the past, was used in the criminal justice system. It was found to disproportionately rate African-American defendants as having a higher risk of reoffending when compared to white defendants. 
          This bias was caused by to historical data and societal inequalities that the model was trained on.
          <br><br>
          The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) tool was intended to predict a defendant’s likelihood of reoffending. It, however, became a glaring example of how algorithmic bias can unintentionally reinforce systemic inequities. A 2016 ProPublica investigation revealed that the algorithm disproportionately flagged Black defendants as high risk nearly twice as often as white defendants who had similar records and outcomes. In addition, white defendants were more likely to be incorrectly labeled as low risk, even when they later reoffended.
  <br><br>
  The data used to train COMPAS was the root of the problem. The system relied on historical criminal justice data which had racial disparities, such as over-policing in predominantly Black communities. Since arrests were treated as a key input, the algorithm unintentionally learned to replicate the same systemic biases embedded in that data.
  <br>
  <br>
  This bias had very real and severe consequences. Judges used COMPAS to inform decisions about bail, parole, and sentencing. They often placed undue burdens on Black defendants labeled as high risk. These decisions could lead to longer sentences, fewer opportunities for rehabilitation, and cycles of incarceration that perpetuated inequality. The case illustrates how bias can creep into systems designed with good intentions, leading to unintentional but devastating outcomes.
  <br>
  <br>
  Preventing such biases requires a more deliberate approach to data selection, ensuring it is representative and free of bias. Algorithms should be rigorously tested for disparities in their outputs, and their decision-making processes should be transparent to all stakeholders. 
  <br>
  <br>
  Source: Angwin, Julia, et al. "Machine Bias." ProPublica, May 23, 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.
  
        </p>
        <h3>Case Study 2: Gender Bias in Hiring</h3>
        <p>
          A hiring algorithm used by a major tech company was found to perpetuated gender inequality in hiring practices.
          <br><br>
          This algorithm became a textbook example of how bias in AI can unintentionally perpetuate inequality. It was designed to automate the recruitment process, however, the algorithm downgraded resumes from women while favoring those from men, even when qualifications were very similar. This wasn’t an intentional bias, however, it was caused by the algorithm’s training data, which reflected the company’s historical hiring patterns. Because men have been overwhelmingly hired for technical roles in the past, the system learned to associate male-dominated language and experiences with stronger candidates.
  
          <br><br>For example, resumes that mentioned participation in women’s organizations were penalized, while those featuring male-coded activities were more likely to be recommended. Qualified women were less likely to  get interviews. The algorithm reinforced stereotypes about who belongs in tech and slowed progress toward creating more diverse, inclusive workplaces.
  
          <br><br>This case shows how easily bias can creep into AI systems when historical inequalities go unchecked. Preventing this requires more than good intentions. Training data must be balanced and representative, algorithms need to be audited for fairness, and systems should include feedback loops to catch and correct biases early.
  
          <br><br>Source: Dastin, Jeffrey. "Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women." Reuters, October 10, 2018, https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.
        </p>
      </section>
    <section>
      <h2>How to Prevent Bias in AI</h2>
      <ul>
        <li><strong>Diverse Datasets:</strong> Ensure training datasets represent all demographics fairly.</li>
        <li><strong>Algorithm Auditing:</strong> Regularly audit algorithms for unintended biases.</li>
        <li><strong>Human Oversight:</strong> Involve ethicists and diverse stakeholders in AI development.</li>
        <li><strong>Transparency:</strong> Clearly document data sources and decision-making processes.</li>
      </ul>
      <p>Preventing bias in AI begins with acknowledging that bias exists and understanding its potential impact. Recognizing the problem allows developers and stakeholders to approach AI design with intention and care. To mitigate bias, it’s essential to use diverse and representative datasets, audit algorithms regularly for fairness, and involve multidisciplinary teams in development.</p>
    </section>
  </main>

  <footer>
    <p>© 2024 Sashank Gadisetti E125 Prototype. All Rights Reserved.</p>
  </footer>

  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script>
    let chart; // Variable to store the chart instance
    const candidates = generateCandidates(100); // Generate candidates once

    function generateCandidates(number) {
        const candidates = [];
        for (let i = 0; i < number; i++) {
            candidates.push({
                name: `Candidate ${i + 1}`,
                gender: Math.random() > 0.5 ? 'Male' : 'Female',
                qualified: Math.random() > 0.2, // 80% chance of being qualified
            });
        }
        return candidates;
    }

    function simulate() {
        const bias = parseFloat(document.getElementById('bias').value);
        const results = { Male: 0, Female: 0 };

        candidates.forEach(candidate => {
            const random = Math.random();
            if (candidate.qualified) {
                const biasFactor = candidate.gender === 'Male' ? bias : 1 - bias;
                if (random < biasFactor) {
                    results[candidate.gender]++;
                }
            }
        });

        updateOutput(results);
        updateChart(results);
    }

    function updateOutput(results) {
        const outputDiv = document.getElementById('output');
        outputDiv.innerHTML = `
          <p><strong>Simulation Results:</strong></p>
          <p>Male Candidates Hired: ${results.Male}</p>
          <p>Female Candidates Hired: ${results.Female}</p>
        `;
    }

    function updateChart(results) {
        const ctx = document.getElementById('chart').getContext('2d');

        // Destroy the existing chart instance if it exists
        if (chart) {
            chart.destroy();
        }

        // Create a new chart instance
        chart = new Chart(ctx, {
            type: 'bar',
            data: {
                labels: ['Male', 'Female'],
                datasets: [
                    {
                        label: 'Candidates Hired',
                        data: [results.Male, results.Female],
                        backgroundColor: ['#3f51b5', '#ff5722'],
                    },
                ],
            },
            options: {
                scales: {
                    y: { beginAtZero: true },
                },
            },
        });
    }
  </script>
</body>
</html>
